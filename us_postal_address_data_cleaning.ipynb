{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd5164bc-4fa1-4f2f-ac90-3c44e3583c63",
   "metadata": {},
   "source": [
    "# Cleaning US postal address data\n",
    "\n",
    "22 09 24\n",
    "\n",
    "---\n",
    "\n",
    "US postal address data are patchy in terms of coverage and also messy, with large numbers of errors in postcodes, town names and street names. This notebook goes through steps to clean-up (to some degree) the US postal address data from OpenAddresses. Based on the number of addresses in this dataset it seems to be the most comprehensive source of US address data.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1) Load data from OpenAddresses and filter to the required state, e.g., 'CA' for California\n",
    "2) Impute the city name based on the name of the file\n",
    "3) Append the state name\n",
    "4) For each point (latitude, longitude value) find the nearest road\n",
    "5) Create an imputed_road column to fix any incorrect road assignments in the OpenAddresses file\n",
    "\n",
    "## References\n",
    "\n",
    "- US Census places shapefile: https://www.census.gov/cgi-bin/geo/shapefiles/index.php\n",
    "- US postcodes shapefile: https://www.census.gov/cgi-bin/geo/shapefiles/index.php?year=2020&layergroup=ZIP+Code+Tabulation+Areas\n",
    "- OpenAddresses: https://openaddresses.io/\n",
    "\n",
    "## Other data sources\n",
    "https://www.arcgis.com/home/item.html?id=2202c1cd6708441f987ca5552f2d9659"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5020ddb4-c4e9-4bd2-bba4-1a8b191a6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from re import match\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3be0757-d9d4-48bb-8240-a3506d38b206",
   "metadata": {},
   "source": [
    "### OpenAddresses Data Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "351d45fc-e84a-4495-b06f-c09c3fd9a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_from_filename(filename):\n",
    "    if 'of' in filename:\n",
    "        town_name = filename.split('/')[-1].split('of')[1].split('-')[0][1:].replace('_', ' ').upper()\n",
    "    elif 'statewide' not in filename:\n",
    "        town_name = filename.split('-add')[0].replace('_', ' ').upper()\n",
    "    else:\n",
    "        town_name = ''\n",
    "    return town_name\n",
    "\n",
    "def valid_postcode(postcode_string):\n",
    "    \"\"\"\n",
    "    Check to see if postcode is valid 5 digit us postcode\n",
    "    \"\"\"\n",
    "    try:\n",
    "        postcode_clean = int(float(postcode_string))\n",
    "        if (postcode_clean == float(postcode_string)):\n",
    "            if match('^[0-9]{5}$', str(postcode_clean)):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "73ea0d16-d092-41ab-abed-ba2c83f5aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_address_file(filenames, state_name, postcodes_filename, places_file):\n",
    "    \"\"\"\n",
    "    Given a list of files for a given state, extract the relevant columns and join to postcode\n",
    "    and places files to get city and postcode names\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    filenames (list of str): list of filenames for only a single US state\n",
    "    state_name (str): name of the state (abbreviated)\n",
    "    postcodes_filename (str): path of postcodes file for the state\n",
    "    places_file (str): path of places file for the state\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "\n",
    "    df_all (pd.DataFrame): combined address data for the state\n",
    "    \"\"\"\n",
    "    gdfs = []\n",
    "\n",
    "    print(f\"Loading postcode file: {postcodes_filename}\")\n",
    "    gdf_postcodes = gpd.read_file(postcodes_filename)\n",
    "    \n",
    "    print(f\"Loading cities file: {places_filename}\")\n",
    "    gdf_places = gpd.read_file(places_filename)\n",
    "    \n",
    "    for filename in filenames:\n",
    "        print(f'Loading address data for: {filename}')\n",
    "        # get the city name from the filename\n",
    "        town_name = city_from_filename(filename)\n",
    "        \n",
    "        gdf = (\n",
    "            gpd\n",
    "            .read_file(f'{folder_name}/{filename}')\n",
    "            .assign(latitude=lambda df_: df_.geometry.y,\n",
    "                    longitude=lambda df_: df_.geometry.x,\n",
    "                    state=state_name,\n",
    "                    city2=town_name)\n",
    "            .loc[:, ['state', 'number', 'street', 'unit', 'city', 'city2', 'district', 'postcode', 'latitude', 'longitude', 'geometry']]\n",
    "            .to_crs(gdf_postcodes.crs)\n",
    "        )\n",
    "        # join to postcodes file to get postcode\n",
    "        gdf_new = (\n",
    "            gpd\n",
    "            .sjoin(gdf, gdf_postcodes, how='left', predicate='within')\n",
    "            .loc[:, ['state', 'number', 'street', 'unit', 'city', 'city2', 'district', 'postcode', 'latitude', 'longitude', 'geometry', 'ZCTA5CE20']]\n",
    "        )\n",
    "        # join to places file to get city\n",
    "        gdf_new = (\n",
    "            gpd\n",
    "            .sjoin(gdf_new, gdf_places, how='left', predicate='within')\n",
    "            .loc[:, ['state', 'number', 'street', 'unit', 'city', 'city2', 'district', 'postcode', 'latitude', 'longitude', 'ZCTA5CE20', 'NAME', 'geometry']]\n",
    "        )\n",
    "        gdfs.append(gdf_new)\n",
    "        \n",
    "    df_all = pd.concat(gdfs)\n",
    "    return df_all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e7677fdf-a97e-4158-8e72-0a41a23ddafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load geojson\n",
    "state_name = 'ca'\n",
    "folder_name = f'/Users/alexlee/Desktop/Data/geo/open_addresses/{state_name}'\n",
    "\n",
    "filenames = [filename for filename in listdir(folder_name) if (filename.endswith('json') and 'address' in filename)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b434118-b2af-4200-b069-b7928173837b",
   "metadata": {},
   "source": [
    "## Wrangle US state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37cfe5d7-c224-419d-a88f-229670c1f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "postcodes_filename = '/Users/alexlee/Desktop/Data/geo/us/shapefiles/tl_2020_us_zcta520/tl_2020_us_zcta520.shp'\n",
    "places_folder = '/Users/alexlee/Desktop/Data/geo/us/shapefiles/places/california/'\n",
    "\n",
    "places_file = [filename for filename in listdir(places_folder) if filename.endswith('.shp')]\n",
    "places_filename = f'{places_folder}{places_file[0]}' # set the filename of the shapefile for the state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f1503ed-45b6-422a-9695-69a2e4fd8c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the data\n",
    "addresses_all = wrangle_address_file(filenames, state_name.upper(), postcodes_filename, places_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2324aa15-dd16-4f70-ad84-910515fcd751",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_all.query('street == \"\"').sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "383c7f70-0e93-4f99-9467-2af1542ccf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no city, no postcode\n",
    "addresses1 = (\n",
    "    addresses_all\n",
    "    .query('city.str.strip() == \"\"').query('postcode.str.strip() == \"\"')\n",
    "    .assign(city=lambda df_: df_.city2,\n",
    "            postcode=lambda df_: df_.ZCTA5CE20)\n",
    "    .loc[:, ['number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'geometry']]\n",
    ")\n",
    "\n",
    "# no city, has postcode\n",
    "addresses2 = (\n",
    "    addresses_all\n",
    "    .query('city.str.strip() == \"\"').query('postcode.str.strip() != \"\"')\n",
    "    .assign(city=lambda df_: df_.city2)\n",
    "    .loc[:, ['number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'geometry']]\n",
    ")\n",
    "\n",
    "# has city, no postcode\n",
    "addresses3 = (\n",
    "    addresses_all\n",
    "    .query('city.str.strip() != \"\"').query('postcode.str.strip() == \"\"')\n",
    "    .assign(postcode=lambda df_: df_.ZCTA5CE20)\n",
    "    .loc[:, ['number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'geometry']]\n",
    ")\n",
    "\n",
    "# has city, has postcode\n",
    "addresses4 = (\n",
    "    addresses_all\n",
    "    .query('city.str.strip() != \"\"').query('postcode.str.strip() != \"\"')\n",
    "    .loc[:, ['number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'geometry']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1720624c-d12d-4a69-b740-ed8b9b99fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put these all together and create the full address string\n",
    "addresses_all_final = (\n",
    "    pd\n",
    "    .concat([addresses1, addresses2, addresses3, addresses4])\n",
    "    .drop_duplicates()\n",
    "    .assign(full_address=lambda df_: df_.number + ' ' + df_.street.str.upper() + ' ' + df_.unit + ' ' + df_.city.str.upper() + ' ' + df_.state.str.upper() + ' ' + df_.postcode)\n",
    "    .query('postcode.isnull() == False')\n",
    "    .assign(rowid=lambda df_: np.arange(df_.shape[0]))\n",
    "    .reset_index().iloc[:, 1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ab416-c730-477e-84a4-cb931f952a88",
   "metadata": {},
   "source": [
    "## Fix postcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e6a7fed-def8-4d25-9be1-f052bcb071ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check which ones have valid postcodes\n",
    "valid_postcodes = addresses_all_final.postcode.apply(valid_postcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b02550a4-4068-4da2-be11-5a351b730f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addresses that do not have a valid postcode\n",
    "# use USPS postcode derived from lat, lng values\n",
    "good_addresses = addresses_all_final.loc[valid_postcodes, :]\n",
    "bad_addresses = addresses_all_final.loc[valid_postcodes == False, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "836f518b-6f35-4910-b84a-f328c283f05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306472, 11)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_addresses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e209677-6297-4448-b4d4-37c987920d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_postcodes = gpd.read_file(postcodes_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4eedf5c-7e80-4996-bfdd-0cab02348ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_addresses = gpd.sjoin(bad_addresses, gdf_postcodes, how='left', predicate='within')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf3a18b9-b749-46a3-b938-da628d2e8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_addresses_cleaned = (\n",
    "    bad_addresses\n",
    "    .assign(postcode=lambda df_: df_.ZCTA5CE20)\n",
    "    .assign(full_address=lambda df_: df_.number + ' ' + df_.street.str.upper() + ' ' + df_.unit + ' ' + df_.city.str.upper() + ' ' + df_.state.str.upper() + ' ' + df_.postcode)\n",
    "    .loc[:, ['number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'full_address', 'geometry']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "391e5cd8-5865-42a6-bca1-36d32ce90826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the good with the bad\n",
    "addresses_all_final = pd.concat([good_addresses, bad_addresses_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2115225e-2140-42d2-a2e5-d40e322c90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_all_final = addresses_all_final.query('postcode.isnull() == False')\n",
    "addresses_all_final = addresses_all_final.assign(postcode=lambda df_: df_.postcode.astype(float).astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6624bc15-2117-4371-8a74-ae881353d116",
   "metadata": {},
   "source": [
    "## Fix bad street names\n",
    "\n",
    "E.g., '03rd st' instead of '3rd street'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a91d6-b8c1-4201-a7c9-75f1722a0fa1",
   "metadata": {},
   "source": [
    "### Read the shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4d11e2c7-a98b-41cf-aab0-28635571b896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all the roads shapefiles\n",
    "filenames = [filename for filename in listdir('/Users/alexlee/Desktop/Data/geo/us/shapefiles/all_roads/') if filename.endswith('.shp')]\n",
    "folder_name = '/Users/alexlee/Desktop/Data/geo/us/shapefiles/all_roads'\n",
    "\n",
    "gdfs = []\n",
    "\n",
    "# extract all the roads data\n",
    "for road_filename in filenames:\n",
    "    print(f\"Reading {road_filename}\")\n",
    "    filename = f'{folder_name}/{road_filename}'\n",
    "    gdf = gpd.read_file(filename).query('FULLNAME.isnull() == False')\n",
    "    gdfs.append(gdf)\n",
    "\n",
    "gdf_roads = gpd.GeoDataFrame(pd.concat(gdfs, ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c48bad-6964-4451-8254-452a98ca8d63",
   "metadata": {},
   "source": [
    "### Append new road name to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093256cd-1997-4d07-a0ea-97c611248971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the postal address data to a GeoDataFrame\n",
    "gdf_addresses = (\n",
    "    gpd\n",
    "    .GeoDataFrame(\n",
    "        addresses_all_final\n",
    "        .assign(geometry=gpd.points_from_xy(x=addresses_all_final.longitude, y=df_ca.latitude, crs=gdf_all.crs)))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c607fa7-518b-4e1b-9ad9-d85bed1123cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the nearest road to each of the postal addresses\n",
    "gdf_nearest = (\n",
    "    gpd\n",
    "    .sjoin_nearest(gdf_ca.to_crs('EPSG:3310'), gdf_all.to_crs('EPSG:3310'), how='left', max_distance=None)\n",
    "    .loc[:, ['rowid', 'number', 'street', 'unit', 'city', 'state', 'postcode',\n",
    "             'latitude', 'longitude', 'full_address', 'index_right',\n",
    "             'LINEARID', 'FULLNAME', 'RTTYP', 'MTFCC']]\n",
    ")\n",
    "\n",
    "roads_grouped = gdf_nearest.loc[:, ['street', 'postcode', 'FULLNAME']].groupby(['street', 'postcode', 'FULLNAME'], as_index=False).size()\n",
    "\n",
    "roads_best_match = (\n",
    "    roads_grouped\n",
    "    .sort_values(by=['street', 'postcode', 'size'], ascending=False)\n",
    "    .groupby(['street', 'postcode'], as_index=False)\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# mapping between the road_postcode name and the road name\n",
    "roads_best_match = roads_best_match.assign(street_postcode=lambda df_: df_.street + '_' + df_.postcode.astype(str))\n",
    "\n",
    "road_to_best_road = dict(roads_best_match.loc[:, ['street_postcode', 'FULLNAME']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8389ed42-892d-40f4-9c2d-c6a69f2b0675",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_all_final = (\n",
    "    addresses_all_final\n",
    "    .assign(street_postcode=lambda df_: df_.street + '_' + df_.postcode.astype(str))\n",
    "    .assign(street2=lambda df_: df_.street_postcode.map(road_to_best_road).str.upper().str.strip())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e00c896-d7ce-47b2-97b3-3424c5f67b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the 'NONE' ones\n",
    "addresses_all_final = addresses_all_final.query('street != \"NONE\"')\n",
    "\n",
    "# remove the 'UNASSIGNED' ones\n",
    "addresses_all_final = addresses_all_final.query('street.str.contains(\"UNASSIGNED\") == False')\n",
    "\n",
    "addresses_all_final = (\n",
    "    addresses_all_final\n",
    "    .query('street != \"0\"')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ede1659-6006-4609-990d-02889efe3557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find where the mismatches are between street and street2\n",
    "mismatches = addresses_all_final.drop_duplicates().query('street != street2')\n",
    "\n",
    "mismatches = (\n",
    "    mismatches\n",
    "    .assign(street=lambda df_: df_.street2)\n",
    "    .assign(full_address=lambda df_: df_.number + ' ' + df_.street.str.upper() + ' ' + df_.unit + ' ' + df_.city.str.upper() + ' ' + df_.state.str.upper() + ' ' + df_.postcode.astype(str))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9cb0d836-6180-4116-9cae-35ab6baeec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_all_cleaned = (\n",
    "    pd.concat([mismatches, addresses_all_final.drop_duplicates()])\n",
    "    .reset_index()\n",
    "    .iloc[:, 1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a783c32a-f3b9-4416-8daf-7d4bbaf86253",
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses_all_cleaned = addresses_all_cleaned.assign(rowid=np.arange(addresses_all_cleaned.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4aeab6e-9707-41af-b5e0-a9b2a0058f6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25288007, 12)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addresses_all_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a72d5651-8166-44ad-880d-71a128e8c1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the additional addresses where there is a mismatch between street and street2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701ca872-9864-44a0-a71e-81821a0c151e",
   "metadata": {},
   "source": [
    "## Write the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a703e6e5-6cb9-4b8d-8f5f-1c1a478e16aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the results\n",
    "(\n",
    "    addresses_all_cleaned\n",
    "    .loc[:, ['rowid', 'number', 'street', 'unit', 'city', 'state', 'postcode', 'latitude', 'longitude', 'full_address']]\n",
    "    .to_parquet('/Users/alexlee/Desktop/Data/geo/us/open_addresses_cleaned/ca_cleaned_190924.parquet')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e64544c-9f1a-44b4-a554-e1cd520b6bcd",
   "metadata": {},
   "source": [
    "## Spatial joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "7b4c8c5b-fad1-4826-81c6-a6e82b57b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_places = gpd.read_file(places_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f859ef7-38ec-476a-b5d1-64001f4d21aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_places.query('NAME == \"Woodbourne\"').geometry.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061e13fa-6042-4772-afff-f444bffae445",
   "metadata": {},
   "source": [
    "## Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "fcaba63a-8933-41aa-a44a-841755cf81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b728d-0f79-4c7e-89ab-9703a731cac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one polygon (e.g., by index)\n",
    "#selected_polygon = shapefile.query('NAME == \"Langhorne\"')\n",
    "roads_polygon = roads_shapefile.query(\"FULLNAME == 'Glendale St'\")\n",
    "\n",
    "# Get the centroid of the selected polygon to initialize the map\n",
    "centroid = roads_polygon.geometry.centroid\n",
    "map_center = [centroid.y.mean(), centroid.x.mean()]\n",
    "\n",
    "# Initialize the map centered around the selected polygon\n",
    "my_map = folium.Map(location=map_center, zoom_start=16)\n",
    "\n",
    "# Add selected polygon to the map\n",
    "folium.GeoJson(roads_polygon).add_to(my_map)\n",
    "\n",
    "# Display the map in the notebook\n",
    "my_map\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "geo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
